{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "777f2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from futureturngpt.model import TurnGPT, TurnGPTWandbCallbacks\n",
    "from os.path import join\n",
    "import re\n",
    "import pdb\n",
    "from futureturngpt.plot_utils import plot_trp\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a58758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "LOAD CHECKPOINT TOKENIZER\n",
      "Loaded tokenizer\n",
      "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1e+30, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<ts>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<speaker1>', '<speaker2>', '<future>']})\n",
      "Resized weights\n",
      "######################################################################\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model_type = 'futureturngpt'\n",
    "\n",
    "future_turngpt = True\n",
    "if future_turngpt:\n",
    "    chpt_path = '../runs/futureTurnGPT/futureTurnGPT_3qrpr59m_epoch=3_val_loss=0.2666.ckpt'\n",
    "    chpt = join(\n",
    "            chpt_path\n",
    "        )\n",
    "    model = TurnGPT.load_from_checkpoint(chpt)\n",
    "else:\n",
    "    from original_turngpt.model import originalTurnGPT, originalTurnGPTWandbCallbacks\n",
    "    chpt = join(\n",
    "        \"/home/binger/repos/TurnGPT/runs/TurnGPT/TurnGPT_2cr6pudn/epoch=10_val_loss=1.7908.ckpt\"\n",
    "    )\n",
    "    model =  originalTurnGPT.load_from_checkpoint(chpt)\n",
    "turn_list = [\n",
    "        #['What did you do yesterday?', 'I went hiking with my friends John and Mary. What did you do yesterday?', 'I was painting the wall of my garage.'],\n",
    "        #['What did you do yesterday?', 'I went hiking with my friends John and Mary.', 'That sounds cool!'],\n",
    "        #['What did you do yesterday?', 'I went hiking with my friends John and Mary. What did you do yesterday?', 'That sounds cool!'],\n",
    "        #['What did you do yesterday?', 'I went hiking with my friends John and Mary. What did you do yesterday?', 'Are you okay?'],\n",
    "        #['What did you order?', 'What we always have for brunch here, tuna sandwiches, fries, pudding, and coffee.', 'That sounds good!'],\n",
    "        #['What did you order?', 'What we always have for brunch here, tuna sandwiches, fries, pudding, and coffee.', 'Are you okay?'],\n",
    "        #['What did you order?', 'What we always have for brunch here, do you want anything else today?', 'No, that sounds good!']\n",
    "        #['Did you and Mary meet yesterday?','Yes, we met in the park.',\"That's great, I'm glad to hear that! when will you meet again?\",'tomorrow.']\n",
    "        ['Yesterday we met in the park.','Okay, when will you meet again?','tomorrow.']\n",
    "        #[\"peter enough with your computer games go do your homework now\",\"can't i play more?\",\"no, stop playing computer games\", \"mom i'll be finished soon\",\"peter, if you don't turn off your computer then i won't allow you to play it again starting next week.\"]\n",
    "        #[\"hi brittany, what are you doing with all of your clothes on your bed?\",\"i'm trying to decide what to wear to school the first day\",\"oh mom didn't tell you?\",\" didn't tell me what?\",\" this school you're going to is going to make your life easy\",\" what are you talking about brother spill it\",\" uniforms sis, no more worrying about appearances\",\"you mean i have to wear the same thing every day mom\"]\n",
    "        #[\"hello sir i'm ready for you\",\"is it my turn?\",\"yes, please sit on the chair. how do you want to have your hair cut?\",\"not too long, cut a little off behind and on both sides too\",\"ok, now lean back a little and keep still. i'm going to shave your face.\"]\n",
    "    ]\n",
    "\n",
    "model.transformer.config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75cab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_list = [\n",
    "    [\"Hi, I'd like to book an appointment with Dr. X.\",\n",
    "     \"I'm sorry, Dr. X isn't available this month. Would you like to book an appointment with Dr. Y or Dr. Z instead?\",\n",
    "     \"Are there any other doctors available this month?\"\n",
    "    ],\n",
    "    [\"Hi, I'd like to book an appointment with Dr. X.\",\n",
    "     \"I'm sorry, Dr. X isn't available this month. Would you like to book an appointment with Dr. Y or Dr. Z instead?\",\n",
    "     \"I'd like to book an appointment with Dr. Y then, thank you.\"\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574b2965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binger/repos/futureTurnGPT/futureturngpt/model.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp_inp.append(torch.tensor(inp))\n",
      "/home/binger/repos/futureTurnGPT/futureturngpt/model.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp_sp.append(torch.tensor(sp))\n"
     ]
    }
   ],
   "source": [
    "out = model.string_list_to_trp_inputid(turn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11cbd808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'mc_loss', 'logits', 'mc_logits', 'past_key_values', 'attentions', 'probs', 'trp_probs', 'tokens', 'input_ids', 'trp_proj'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d29cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = out['attentions'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1240fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 51])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = out['input_ids']\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3ef846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  389,   612,   597,   584,  7519,  1695,   428,  1227, 50257,  5303,\n",
       "         1312,  1549,   588,   284,  1492,   281, 12557,   351,  1553,  2124,\n",
       "        50257,  1312,  1101,  7926,  1553,  2124,  2125,   470,  1695,   428,\n",
       "         1227,   561,   345,   588,   284,  1492,   281, 12557,   351,  1553,\n",
       "          331,   393,  1553,  1976,  2427, 50257, 50256, 50256, 50256, 50256,\n",
       "        50256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f7d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1312,  1549,   588,   284,  1492,   281, 12557,   351,  1553,   331,\n",
       "          788,  5875,   345, 50257,  5303,  1312,  1549,   588,   284,  1492,\n",
       "          281, 12557,   351,  1553,  2124, 50257,  1312,  1101,  7926,  1553,\n",
       "         2124,  2125,   470,  1695,   428,  1227,   561,   345,   588,   284,\n",
       "         1492,   281, 12557,   351,  1553,   331,   393,  1553,  1976,  2427,\n",
       "        50257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c684ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_idx = [(sublist == model.tokenizer.eos_token_id).nonzero(as_tuple=True)[0] for sublist in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b0345b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 8, 20, 45]), tensor([13, 25, 50])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts_idx[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn=attns[0,:,ts,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attns[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(attns[0,0,-1,:ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(attns[0,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07c90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def percent_future_attention (attns,input_ids):\n",
    "    '''\n",
    "    attns = [batch_size, n_head, len_dialog, len_dialog]\n",
    "    '''\n",
    "    future_attentions = [] # one value for each dialog (average future attention by head)\n",
    "    future_attentions_all_heads = [] # n_head value for each dialog\n",
    "    ts_idx = [(sublist == model.tokenizer.eos_token_id).nonzero(as_tuple=True)[0] for sublist in input_ids]\n",
    "    n_heads = attns.shape[1]\n",
    "    for i, attn in enumerate(attns):\n",
    "        future_ts = ts_idx[i][0]\n",
    "        attn_future_all = [sum(attns[i,j,-1,:future_ts]) for j in range(12)]\n",
    "        future_attentions.append(sum(attn_future_all)/n_heads)\n",
    "        future_attentions_all_heads.append(attn_future_all)\n",
    "    \n",
    "    return torch.FloatTensor(future_attentions), torch.FloatTensor(future_attentions_all_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff074cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_attns, fut_attns_heads = percent_future_attention(attns, out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea98b5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7434, 0.2566, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6699, 0.1868, 0.1434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6337, 0.1404, 0.1352, 0.0908, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7246, 0.0705, 0.0801, 0.0780, 0.0468, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7266, 0.0350, 0.0313, 0.0304, 0.1016, 0.0752, 0.0000, 0.0000],\n",
       "        [0.6820, 0.0204, 0.0325, 0.0358, 0.1090, 0.0737, 0.0466, 0.0000],\n",
       "        [0.6499, 0.0313, 0.0284, 0.0258, 0.1015, 0.0946, 0.0287, 0.0398],\n",
       "        [0.5167, 0.0594, 0.0414, 0.0438, 0.0827, 0.0597, 0.0190, 0.0590],\n",
       "        [0.6136, 0.0369, 0.0267, 0.0251, 0.0404, 0.0279, 0.0151, 0.0378],\n",
       "        [0.5373, 0.0298, 0.0266, 0.0271, 0.0610, 0.0552, 0.0118, 0.0807],\n",
       "        [0.5476, 0.0364, 0.0345, 0.0309, 0.0598, 0.0293, 0.0122, 0.0455],\n",
       "        [0.5046, 0.0386, 0.0354, 0.0318, 0.0838, 0.0594, 0.0163, 0.0529],\n",
       "        [0.5057, 0.0276, 0.0216, 0.0183, 0.0843, 0.0605, 0.0109, 0.0491],\n",
       "        [0.5916, 0.0223, 0.0168, 0.0126, 0.0501, 0.0328, 0.0121, 0.0399],\n",
       "        [0.5997, 0.0103, 0.0081, 0.0056, 0.0606, 0.0320, 0.0049, 0.0343],\n",
       "        [0.6462, 0.0209, 0.0205, 0.0175, 0.0346, 0.0234, 0.0135, 0.0252],\n",
       "        [0.6049, 0.0186, 0.0102, 0.0081, 0.0436, 0.0269, 0.0068, 0.0231],\n",
       "        [0.7406, 0.0106, 0.0085, 0.0074, 0.0164, 0.0121, 0.0035, 0.0105],\n",
       "        [0.7146, 0.0132, 0.0096, 0.0099, 0.0245, 0.0198, 0.0043, 0.0114],\n",
       "        [0.3676, 0.0336, 0.0180, 0.0144, 0.0264, 0.0221, 0.0098, 0.0211],\n",
       "        [0.5423, 0.0118, 0.0093, 0.0084, 0.0165, 0.0170, 0.0051, 0.0169],\n",
       "        [0.5477, 0.0091, 0.0079, 0.0067, 0.0199, 0.0181, 0.0063, 0.0170],\n",
       "        [0.3898, 0.0200, 0.0139, 0.0112, 0.0131, 0.0115, 0.0161, 0.0169],\n",
       "        [0.3969, 0.0212, 0.0114, 0.0088, 0.0088, 0.0100, 0.0060, 0.0126],\n",
       "        [0.4726, 0.0192, 0.0142, 0.0138, 0.0249, 0.0203, 0.0113, 0.0158],\n",
       "        [0.5488, 0.0205, 0.0185, 0.0172, 0.0127, 0.0197, 0.0121, 0.0180],\n",
       "        [0.4443, 0.0102, 0.0076, 0.0063, 0.0256, 0.0292, 0.0076, 0.0187],\n",
       "        [0.3822, 0.0159, 0.0105, 0.0087, 0.0166, 0.0187, 0.0225, 0.0161],\n",
       "        [0.5214, 0.0046, 0.0062, 0.0074, 0.0256, 0.0159, 0.0184, 0.0604],\n",
       "        [0.2451, 0.0211, 0.0103, 0.0067, 0.0126, 0.0192, 0.0089, 0.0044],\n",
       "        [0.4137, 0.0157, 0.0106, 0.0138, 0.0193, 0.0123, 0.0090, 0.0130],\n",
       "        [0.4845, 0.0154, 0.0103, 0.0123, 0.0154, 0.0144, 0.0059, 0.0183],\n",
       "        [0.3095, 0.0129, 0.0095, 0.0069, 0.0265, 0.0213, 0.0069, 0.0173],\n",
       "        [0.3600, 0.0139, 0.0085, 0.0056, 0.0315, 0.0214, 0.0053, 0.0190],\n",
       "        [0.3480, 0.0149, 0.0084, 0.0062, 0.0182, 0.0179, 0.0103, 0.0261],\n",
       "        [0.4747, 0.0103, 0.0057, 0.0043, 0.0211, 0.0224, 0.0048, 0.0244],\n",
       "        [0.4315, 0.0217, 0.0129, 0.0125, 0.0156, 0.0177, 0.0113, 0.0224],\n",
       "        [0.4241, 0.0130, 0.0070, 0.0048, 0.0234, 0.0134, 0.0037, 0.0143],\n",
       "        [0.4509, 0.0145, 0.0061, 0.0052, 0.0072, 0.0079, 0.0035, 0.0084],\n",
       "        [0.6905, 0.0051, 0.0030, 0.0051, 0.0137, 0.0077, 0.0027, 0.0037],\n",
       "        [0.3399, 0.0143, 0.0095, 0.0063, 0.0231, 0.0119, 0.0031, 0.0125],\n",
       "        [0.4445, 0.0127, 0.0051, 0.0042, 0.0070, 0.0095, 0.0031, 0.0100],\n",
       "        [0.6988, 0.0044, 0.0028, 0.0050, 0.0107, 0.0074, 0.0031, 0.0041],\n",
       "        [0.5358, 0.0067, 0.0053, 0.0066, 0.0200, 0.0091, 0.0075, 0.0089],\n",
       "        [0.1590, 0.0208, 0.0106, 0.0073, 0.0116, 0.0091, 0.0043, 0.0073],\n",
       "        [0.5803, 0.0297, 0.0234, 0.0211, 0.0139, 0.0068, 0.0068, 0.0059],\n",
       "        [0.5466, 0.0285, 0.0224, 0.0199, 0.0112, 0.0053, 0.0060, 0.0047],\n",
       "        [0.5162, 0.0270, 0.0211, 0.0188, 0.0094, 0.0044, 0.0054, 0.0039],\n",
       "        [0.4877, 0.0254, 0.0199, 0.0176, 0.0081, 0.0038, 0.0049, 0.0034],\n",
       "        [0.4635, 0.0239, 0.0187, 0.0165, 0.0072, 0.0034, 0.0045, 0.0030]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(attns[0,:,:,:8],0)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ad6684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_attn = torch.sum(attns[0,:,:,:8],0)/attns.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28120cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  389,   612,   597,   584,  7519,  1695,   428,  1227, 50257])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4705fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_attn_by_tok = sum(average_attn, 0)/average_attn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c1ae8805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5289, 0.0310, 0.0208, 0.0159, 0.0297, 0.0218, 0.0087, 0.0188])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fut_attn_by_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340b794",
   "metadata": {},
   "source": [
    "the fist token always takes too much attention. so it always stops at the first token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d220f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_main_attention (attns, future_attentions,ts_idx, percent = 0.9):\n",
    "    '''\n",
    "    attention: one per each converstation, only last one permutated\n",
    "    [batch_size, n_head, len_dialog, len_dialog]\n",
    "    '''\n",
    "    furthest_indices_all = []\n",
    "    rel_furthest_indices_all = []\n",
    "    future_attentions_thresholds = future_attentions * percent\n",
    "    for j, attn in enumerate(attns):\n",
    "        future_ts = ts_idx[j][0]\n",
    "        current_ts = ts_idx[j][-1]\n",
    "        future_attn_threshold = future_attentions_thresholds[j]\n",
    "        # average over attention heads\n",
    "        # attn = [n_head, len_dialog, len_fut_utt]\n",
    "        average_attn = torch.sum(attn[:,:,:future_ts],0)/attn.shape[0]\n",
    "        # average over y axis\n",
    "        # fut_attn_by_tok = one val for each tok in future utt\n",
    "        #fut_attn_by_tok = sum(average_attn, 0)/average_attn.shape[0]\n",
    "        fut_attn_by_tok = average_attn[current_ts]\n",
    "        if sum(fut_attn_by_tok) < future_attn_threshold:\n",
    "            # doesn't reach threshold at all\n",
    "            furthest_indices_all.append(-1)\n",
    "            rel_furthest_indices_all.append(-1)\n",
    "        else: # reaches threshold at some point\n",
    "            cumulative_attn = 0\n",
    "            i = 0\n",
    "            while i < len(fut_attn_by_tok) and cumulative_attn < future_attn_threshold:\n",
    "                cumulative_attn += fut_attn_by_tok[i]\n",
    "                if cumulative_attn > future_attn_threshold:\n",
    "                    print(cumulative_attn)\n",
    "                    print(future_attentions_thresholds)\n",
    "                    print('-----')\n",
    "                    furthest_indices_all.append(i)\n",
    "                    rel_furthest_indices_all.append(i/len(fut_attn_by_tok))\n",
    "                    break\n",
    "                else:\n",
    "                    i+=1\n",
    "    return torch.FloatTensor(furthest_indices_all), torch.FloatTensor(rel_furthest_indices_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231cd494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3348)\n",
      "tensor([0.4867, 0.3196])\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "furthestidx, relidx = calculate_main_attention(attns, fut_attns, ts_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a12595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1., 11.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furthestidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a330574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000,  0.8462])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3faecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d98195e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4867, 0.3196])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(fut_attns)*0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "attns[0,:,:,:ts_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d2f0967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 51, 8])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attns[0,:,:,:8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410876b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4756be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c1f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d37ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bde7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81377a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c70c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153ed35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e61e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54879c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef873bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(attns[2,0,-1,:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def string_list_to_trp_inputid(self, string_or_list, add_post_eos_token=False, use_label=True,**model_kwargs):\n",
    "    t = self.tokenize_strings(string_or_list, add_post_eos_token=add_post_eos_token)\n",
    "    if use_label:\n",
    "        lm_labels = self.get_labels(t[\"input_ids\"], mask=t['attention_mask'])\n",
    "    proj_labels = None\n",
    "    if self.trp_projection_steps > 0:\n",
    "        proj_labels = self.get_projection_labels(\n",
    "            t[\"input_ids\"], mask=t[\"attention_mask\"]\n",
    "        )\n",
    "    # Model\n",
    "    if not use_label:\n",
    "        out = self(t[\"input_ids\"], speaker_ids=t[\"speaker_ids\"], **model_kwargs)\n",
    "    else:\n",
    "        out = self(t[\"input_ids\"], \n",
    "                   speaker_ids=t[\"speaker_ids\"], \n",
    "                   labels=lm_labels,\n",
    "                   mc_labels=proj_labels,\n",
    "                   **model_kwargs)        \n",
    "    out[\"probs\"] = out[\"logits\"].softmax(dim=-1)\n",
    "    out[\"trp_probs\"] = self.get_trp(out[\"probs\"])\n",
    "    out[\"tokens\"] = self.get_tokens(t[\"input_ids\"])\n",
    "    out['input_ids'] = t['input_ids']\n",
    "    if \"mc_logits\" in out:\n",
    "        out[\"trp_proj\"] = out[\"mc_logits\"].sigmoid()\n",
    "    #pdb.set_trace()\n",
    "    return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "965fe6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = [sum(attns[0,i,-1,:8]) for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da868f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5407)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(heads)/len(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed6964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c750f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(heads)/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8274592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f53b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d01b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942a0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d4544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b05639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ecbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7527b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f3384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd505235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80bf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f21763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996f44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
